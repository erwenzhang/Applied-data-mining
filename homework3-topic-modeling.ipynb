{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hide warnings to keep things tidy.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim import matutils, corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pandas as pd\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "\n",
    "import numpy as np \n",
    "import snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_1_data = pd.read_csv(\"merged-democratic-debate-transcripts.txt\", delimiter=\"\\t\",header=None)\n",
    "merge_1_data.columns = [\"instances\"]\n",
    "merge_2_data = pd.read_csv(\"merged-republican-debate-transcripts.txt\", delimiter=\"\\t\",header=None)\n",
    "merge_2_data.columns = [\"instances\"]\n",
    "merge_data = merge_1_data.append(merge_2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, good evening. And I want to thank the C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You know, I remember well when my youth minist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>And that is our fight still. We have to get th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I understand that this is the hardest job in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(APPLAUSE)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           instances\n",
       "0   Well, good evening. And I want to thank the C...\n",
       "1  You know, I remember well when my youth minist...\n",
       "2  And that is our fight still. We have to get th...\n",
       "3  I understand that this is the hardest job in t...\n",
       "4                                        (APPLAUSE) "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances = merge_data[\"instances\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3081, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of instances = 3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now 'tokenize' these\n",
    "tokenized_docs = []\n",
    "for instance in instances:\n",
    "    # lowercase the words\n",
    "    doc_lowercase = str(instance.lower())\n",
    "    # now tokenize via NLTK\n",
    "    doc_tokens = nltk.tokenize.word_tokenize(doc_lowercase)\n",
    "    # drop stop words, like 'the', 'a', etc.\n",
    "    stop_list = stopwords.words('english')\n",
    "    stop_list.extend(string.punctuation)\n",
    "    stop_list.extend([\"''\",\"...\",\"mdash\",\"/i\",\"``\",\"would\",\"'m\",\"(APPLAUSE)\",\"'s\",\"'re\",\"'ve\",\"'ll\",\"n't\",\"mdash\",\"--\"])\n",
    "    doc_tokens = [word.decode('utf-8') for word in doc_tokens if not word in stop_list]\n",
    "    tokenized_docs.append(doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'understand',\n",
       " u'hardest',\n",
       " u'job',\n",
       " u'world',\n",
       " u'prepared',\n",
       " u'ready',\n",
       " u'take',\n",
       " u'hope',\n",
       " u'earn',\n",
       " u'support',\n",
       " u'nominee',\n",
       " u'democratic',\n",
       " u'party',\n",
       " u'next',\n",
       " u'president',\n",
       " u'united',\n",
       " u'states']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'well', u'good', u'even', u'want', u'thank', u'congression', u'black', u'caucu', u'institut', u'peopl', u'charleston', u'host', u'us', u'eve', u'martin', u'luther', u'king', u'day', u'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "#create a PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "tokenized_and_stemmed = [[p_stemmer.stem(w) for w in doc] for doc in tokenized_docs]\n",
    "print(tokenized_and_stemmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55417\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for toked_data in tokenized_and_stemmed:\n",
    "    count = count+len(toked_data)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(4557 unique tokens: [u'fawn', u'gadhafi', u'rebel', u'pardon', u'pplaus']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_and_stemmed)\n",
    "gensim_corpus = [dictionary.doc2bow(toked_data) for toked_data in tokenized_and_stemmed]\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of words = 55417 #after stemmed and exclude stopwords,\n",
    "\n",
    "\n",
    "number of unique tokens = 4557"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.017*peopl + 0.011*countri + 0.011*go + 0.010*say + 0.009*let + 0.009*know + 0.009*state + 0.008*well + 0.007*immigr + 0.007*think',\n",
       " u'0.017*tax + 0.014*peopl + 0.013*make + 0.011*go + 0.011*pay + 0.008*want + 0.008*percent + 0.007*us + 0.007*know + 0.007*plan',\n",
       " u'0.018*crosstalk + 0.012*laughter + 0.012*marco + 0.011*thank + 0.009*question + 0.009*think + 0.008*first + 0.008*debat + 0.008*said + 0.007*applaus',\n",
       " u'0.019*go + 0.019*peopl + 0.017*applaus + 0.012*want + 0.012*think + 0.011*get + 0.008*countri + 0.007*know + 0.007*isi + 0.006*one',\n",
       " u'0.015*presid + 0.013*need + 0.012*countri + 0.012*go + 0.010*peopl + 0.009*know + 0.009*obama + 0.009*get + 0.008*world + 0.008*state']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ks = [5,10,15]\n",
    "#for k in ks:\n",
    "lda = LdaModel(gensim_corpus, num_topics=5,\n",
    "               passes=10, alpha=0.001, \n",
    "               id2word=dictionary)\n",
    "lda.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.024*isi + 0.016*world + 0.013*go + 0.012*countri + 0.012*need + 0.011*radic + 0.010*one + 0.010*war + 0.010*presid + 0.009*think',\n",
       " u'0.041*applaus + 0.018*go + 0.013*new + 0.012*get + 0.011*back + 0.011*know + 0.010*believ + 0.009*time + 0.009*state + 0.008*want',\n",
       " u'0.017*work + 0.017*know + 0.013*state + 0.011*need + 0.010*american + 0.010*famili + 0.010*make + 0.008*unit + 0.008*time + 0.008*countri',\n",
       " u'0.021*presid + 0.016*know + 0.015*go + 0.015*peopl + 0.009*countri + 0.009*applaus + 0.009*get + 0.009*obama + 0.008*law + 0.008*say',\n",
       " u'0.022*laughter + 0.019*question + 0.014*well + 0.013*let + 0.012*answer + 0.011*ye + 0.011*look + 0.011*think + 0.011*know + 0.011*like',\n",
       " u'0.026*peopl + 0.018*talk + 0.014*think + 0.012*right + 0.011*issu + 0.011*get + 0.011*countri + 0.010*well + 0.010*go + 0.009*say',\n",
       " u'0.031*peopl + 0.011*win + 0.011*well + 0.011*countri + 0.010*think + 0.010*let + 0.010*thank + 0.010*go + 0.010*said + 0.008*applaus',\n",
       " u'0.026*go + 0.018*peopl + 0.017*want + 0.015*health + 0.015*care + 0.013*get + 0.011*wall + 0.011*street + 0.011*take + 0.010*countri',\n",
       " u'0.027*crosstalk + 0.011*need + 0.011*think + 0.009*use + 0.009*get + 0.009*well + 0.008*domain + 0.008*north + 0.008*emin + 0.007*wolf',\n",
       " u'0.033*tax + 0.017*year + 0.014*percent + 0.013*pay + 0.010*make + 0.010*state + 0.009*countri + 0.009*go + 0.009*money + 0.009*jersey']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ks = [5,10,15]\n",
    "#for k in ks:\n",
    "lda = LdaModel(gensim_corpus, num_topics=10,\n",
    "               passes=10, alpha=0.001, \n",
    "               id2word=dictionary)\n",
    "lda.print_topics(num_topics=10, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.021*peopl + 0.017*health + 0.016*care + 0.015*state + 0.012*year + 0.009*insur + 0.007*countri + 0.007*american + 0.007*could + 0.007*pass',\n",
       " u'0.029*wall + 0.028*crosstalk + 0.019*street + 0.013*ye + 0.012*secur + 0.011*obama + 0.011*nuclear + 0.010*go + 0.010*come + 0.010*vote',\n",
       " u'0.016*presid + 0.015*peopl + 0.014*go + 0.012*well + 0.012*know + 0.011*use + 0.011*need + 0.010*obama + 0.010*say + 0.010*never',\n",
       " u'0.077*tax + 0.024*busi + 0.022*pay + 0.020*money + 0.019*plan + 0.018*percent + 0.013*bring + 0.013*rate + 0.012*one + 0.012*social',\n",
       " u'0.023*go + 0.019*right + 0.015*want + 0.012*us + 0.012*need + 0.012*get + 0.011*one + 0.009*differ + 0.009*question + 0.009*answer',\n",
       " u'0.017*year + 0.017*percent + 0.016*ring + 0.016*bell + 0.015*go + 0.011*get + 0.011*applaus + 0.010*want + 0.010*wrong + 0.009*colleg',\n",
       " u'0.027*countri + 0.020*presid + 0.014*go + 0.013*think + 0.013*peopl + 0.011*militari + 0.010*world + 0.009*come + 0.009*american + 0.009*make',\n",
       " u'0.024*go + 0.021*isi + 0.019*think + 0.017*need + 0.015*take + 0.014*win + 0.014*get + 0.011*look + 0.010*group + 0.009*defeat',\n",
       " u'0.032*know + 0.020*well + 0.017*laughter + 0.014*work + 0.012*presid + 0.010*clinton + 0.009*think + 0.009*hillari + 0.009*like + 0.008*need',\n",
       " u'0.028*immigr + 0.017*peopl + 0.016*go + 0.016*illeg + 0.014*legal + 0.014*law + 0.011*support + 0.010*issu + 0.009*like + 0.009*american',\n",
       " u'0.022*go + 0.019*peopl + 0.016*need + 0.013*jersey + 0.013*question + 0.011*countri + 0.010*new + 0.010*tremend + 0.010*tell + 0.010*jeb',\n",
       " u'0.026*peopl + 0.024*make + 0.022*want + 0.018*america + 0.017*believ + 0.017*work + 0.016*countri + 0.011*go + 0.011*american + 0.011*famili',\n",
       " u'0.038*applaus + 0.029*let + 0.024*talk + 0.018*problem + 0.017*enforc + 0.017*law + 0.016*thank + 0.014*want + 0.013*bill + 0.011*need',\n",
       " u'0.025*peopl + 0.019*campaign + 0.016*go + 0.014*know + 0.014*say + 0.013*said + 0.012*get + 0.010*polit + 0.009*much + 0.009*think',\n",
       " u'0.043*state + 0.036*unit + 0.026*presid + 0.018*clinton + 0.017*hillari + 0.017*fight + 0.017*think + 0.013*na + 0.012*gon + 0.012*isi']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LdaModel(gensim_corpus, num_topics=15,\n",
    "               passes=10, alpha=0.001, \n",
    "               id2word=dictionary)\n",
    "lda.print_topics(num_topics=15, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think k = 10 is best for this data. After looking through k=5,k=10 and k=15, k=10 can basically cover all important topics. k=15 has some repeated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenized_instance(instances):\n",
    "    tokenized_docs = []\n",
    "    for instance in instances:\n",
    "    # lowercase the words\n",
    "        doc_lowercase = str(instance.lower())\n",
    "    # now tokenize via NLTK\n",
    "        doc_tokens = nltk.tokenize.word_tokenize(doc_lowercase)\n",
    "    # drop stop words, like 'the', 'a', etc.\n",
    "        stop_list = stopwords.words('english')\n",
    "        stop_list.extend(string.punctuation)\n",
    "        stop_list.extend([\"''\",\"...\",\"mdash\",\"/i\",\"``\",\"would\",\"'m\",\"(APPLAUSE)\",\"'s\",\"'re\",\"'ve\",\"'ll\",\"n't\",\"mdash\",\"--\"])\n",
    "        doc_tokens = [word.decode('utf-8') for word in doc_tokens if not word in stop_list]\n",
    "        tokenized_docs.append(doc_tokens)\n",
    "    return tokenized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_data(tokenized_docs):\n",
    "    p_stemmer = PorterStemmer()\n",
    "    tokenized_and_stemmed = [[p_stemmer.stem(w) for w in doc] for doc in tokenized_docs]\n",
    "    return tokenized_and_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_and_stemmed_1 = stem_data(tokenized_instance(merge_1_data[\"instances\"]))\n",
    "dictionary_1 = corpora.Dictionary(tokenized_and_stemmed_1)\n",
    "gensim_corpus_1 = [dictionary_1.doc2bow(toked_data) for toked_data in tokenized_and_stemmed_1]\n",
    "tokenized_and_stemmed_2 = stem_data(tokenized_instance(merge_2_data[\"instances\"]))\n",
    "dictionary_2 = corpora.Dictionary(tokenized_and_stemmed_2)\n",
    "gensim_corpus_2 = [dictionary_2.doc2bow(toked_data) for toked_data in tokenized_and_stemmed_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.013*countri + 0.013*think + 0.009*peopl + 0.009*need + 0.008*got + 0.008*gun + 0.008*get + 0.008*know + 0.007*say + 0.007*go',\n",
       " u'0.019*peopl + 0.014*go + 0.009*american + 0.008*one + 0.008*state + 0.008*work + 0.008*let + 0.007*make + 0.007*know + 0.007*countri',\n",
       " u'0.016*need + 0.013*know + 0.012*think + 0.010*go + 0.010*peopl + 0.009*street + 0.008*wall + 0.008*american + 0.007*issu + 0.007*talk',\n",
       " u'0.013*care + 0.011*health + 0.011*colleg + 0.010*make + 0.010*get + 0.010*countri + 0.009*peopl + 0.008*afford + 0.008*want + 0.007*go',\n",
       " u'0.023*applaus + 0.019*think + 0.013*peopl + 0.011*well + 0.009*go + 0.009*presid + 0.008*lot + 0.007*know + 0.007*thing + 0.007*get']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_1 = LdaModel(gensim_corpus_1, num_topics=5,\n",
    "               passes=10, alpha=0.001, \n",
    "               id2word=dictionary_1)\n",
    "lda_1.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0.018*isi + 0.017*go + 0.011*need + 0.011*crosstalk + 0.009*peopl + 0.008*get + 0.008*us + 0.007*want + 0.006*terrorist + 0.006*radic',\n",
       " u'0.016*go + 0.015*know + 0.014*peopl + 0.013*want + 0.012*countri + 0.010*get + 0.010*applaus + 0.008*presid + 0.008*think + 0.008*well',\n",
       " u'0.014*go + 0.014*peopl + 0.012*presid + 0.011*state + 0.009*countri + 0.009*let + 0.009*say + 0.009*unit + 0.007*one + 0.007*applaus',\n",
       " u'0.029*tax + 0.009*percent + 0.009*said + 0.009*plan + 0.008*one + 0.008*peopl + 0.007*pay + 0.007*busi + 0.007*go + 0.006*want',\n",
       " u'0.017*peopl + 0.013*presid + 0.012*countri + 0.011*go + 0.009*american + 0.008*back + 0.008*applaus + 0.008*need + 0.006*come + 0.006*tax']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_2 = LdaModel(gensim_corpus_2, num_topics=5,\n",
    "               passes=10, alpha=0.001, \n",
    "               id2word=dictionary_2)\n",
    "lda_2.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both parties focus topics like people, business,country, american,economic,,etc.\n",
    "Besides, Democratic debates have topics like gun,health care and Republican debates have foucses like terrorist, crosstalk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
